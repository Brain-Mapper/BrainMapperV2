{
  "KMeans" : { "algo_info": "Calculates K clusters represented by a central point that is the mean of all the points in a given cluster",
      "algo_usecase" : "General-purpose, even cluster size, flat geometry, not too many clusters. Metric is Euclidean Dist.",
      "param_list" : {
        "init": {
          "param_info" : "The way to initialise the centers : randomly or kmeans++. Randomly chooses k-observations from data for the initial centroids. Kmeans++ selects initial cluster centers in a smart way.",
          "type" : ["random","k-means++"],
          "default" : "random"
        },
        "i_iter" : {
          "param_info" : "The number of iterations you wish to obtain",
          "type" : "int",
          "default" : "1"
        },
        "n_clusters" : {
          "param_info" : "The number of clusters you wish to obtain",
          "type" : "int",
          "default" : "3"
        },
        "score" : {
          "param_info" : "The score choose to calculate the best number of clusters. Calinski-Habaraz score is the relation between the sum of distances squared intergroup and the sum of distances squared intragroup. Whereas, Davies-Bouldin index is the relation between the sum of distances squared intragroup and the sum of distances squared intergroup. The aim is to minimize the sum of distances squared intragroup and to maximize the sum of distances squared intergroup. Smaller is the Davies-Bouldin index and bigger is the Calinski-Habaraz score, better is the number of clusters. Mean silhouette value is between -1 and 1 and the best value is around 1.",
          "type" : ["Calinski-Harabasz", "Davies-Bouldin", "Mean silhouette"],
          "default" : "Mean silhouette"
        },
        "n_init" : {
          "param_info" : "Number of time the k-means algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of inertia.",
          "type" : "int",
          "default" : "1"
        },
        "max_iter" : {
          "param_info": "Maximum number of iterations of the k-means algorithm for a single run",
          "type" : "int",
          "default": "300"
        }
      }
    },

  "KMedoids" : { "algo_info": "KMedoids clustering uses the median value of all the points in a given cluster instead of the mean",
      "algo_usecase" : "General-purpose, even cluster size, flat geometry, not too many clusters.",
      "param_list" : {
        "init": {
          "param_info" : "A way to initialise the centers : randomly or kmeans++. Randomly chooses k-observations from data for the initial centroids. Kmeans++ selects initial cluster centers in a smart way.",
          "type" : ["random","k-means++"],
          "default" : "k-means++"
        },
        "i_iter" : {
          "param_info" : "The number of iterations you wish to obtain",
          "type" : "int",
          "default" : "1"
        },
        "n_clusters" : {
          "param_info" : "The number of clusters you wish to obtain",
          "type" : "int",
          "default" : "3"
        },
        "score" : {
          "param_info" : "The score choose to calculate the best number of clusters. Calinski-Habaraz score is the relation between the sum of distances squared intergroup and the sum of distances squared intragroup. Whereas, Davies-Bouldin index is the relation between the sum of distances squared intragroup and the sum of distances squared intergroup. The aim is to minimize the sum of distances squared intragroup and to maximize the sum of distances squared intergroup. Smaller is the Davies-Bouldin index and bigger is the Calinski-Habaraz score, better is the number of clusters. Mean silhouette value is between -1 and 1 and the best value is around 1.",
          "type" : ["Calinski-Harabasz", "Davies-Bouldin", "Mean silhouette"],
          "default" : "Mean silhouette"
        },
        "metric" : {
          "param_info" : "Metric used to compute the distance between points. It can be 'euclidean', or 'manhattan'. Euclidian is better when data are grouped as a circle and manhattan is better when data are like a square.",
          "type" : ["euclidean","manhattan"],
          "default" : "euclidean"
        }
      }
    },

  "AgglomerativeClustering" : { "algo_info" : "Recursively merges the pair of clusters that minimally increases a given linkage distance (see scikit-learn).",
    "algo_usecase" : "Many clusters, possibly connectivity constraints + \nLarge n_samples and n_clusters",
    "param_list" : {
      "i_iter" : {
        "param_info" : "The number of iterations you wish to obtain",
        "type" : "int",
        "default" : "1"
      },
      "n_clusters" : {
          "param_info" : "The number of clusters you wish to obtain",
          "type" : "int",
          "default" : "2"
      },
      "score" : {
        "param_info" : "The score choose to calculate the best number of clusters. Calinski-Habaraz score is the relation between the sum of distances squared intergroup and the sum of distances squared intragroup. Whereas, Davies-Bouldin index is the relation between the sum of distances squared intragroup and the sum of distances squared intergroup. The aim is to minimize the sum of distances squared intragroup and to maximize the sum of distances squared intergroup. Smaller is the Davies-Bouldin index and bigger is the Calinski-Habaraz score, better is the number of clusters. Mean silhouette value is between -1 and 1 and the best value is around 1.",
        "type" : ["Calinski-Harabasz", "Davies-Bouldin", "Mean silhouette"],
        "default" : "Mean silhouette"
      },
      "affinity" : {
          "param_info" : "Metric used to compute the linkage. It can be 'euclidean', 'l1', 'l2', 'manhattan' or 'cosine'\nIf linkage is 'ward', only 'euclidean' is accepted.",
          "type" : ["euclidean", "l1", "l2","manhattan", "cosine"],
          "default" : "euclidean"
      },
      "linkage" : {
        "param_info": "Which linkage criterion to use : the linkage criterion determines which distance to use between sets of observation.\nThe algorithm will merge the pairs of cluster that minimize this criterion.\n- 'ward' minimizes the variance of the clusters being merged.\n- average uses the average of the distances of each observation of the two sets.\n- complete or maximum linkage uses the maximum distances between all observations of the two sets.",
        "type": ["ward", "complete", "average"],
        "default": "ward"
      }
    }
  },
  "DBSCAN" : { "algo_info": "Density-Based Spatial Clustering of Applications with Noise. Finds core samples of high density and expands clusters from them. Good for data which contains clusters of similar density.",
      "algo_usecase" : "Non-flat geometry, uneven cluster sizes, very large number of samples, medium number of clusters.",
      "param_list" : {
        "eps" : {
          "param_info" : "The maximum distance between two samples for them to be considered as in the same neighborhood",
          "type" : "float",
          "default" : "0.3"
        },
        "min_samples" : {
          "param_info" : "The number of samples (or total weight) in a neighborhood for a point to be considered as a core point. This includes the point itself",
          "type" : "int",
          "default" : "10"
        },
        "metric" : {
          "param_info" : "Metric used to compute the distance between points. It can be 'euclidean', or 'manhattan'",
          "type" : ["euclidean","manhattan"],
          "default" : "euclidean"
        }
      }
    },
    "FuzzyCMeans" : {
        "algo_info" : "Fuzzy CMeans creates K clusters represented by a central point that is the mean of all the points in a given cluster and each points belongs more or less to the cluster. A point can belong to one or many cluster(s).",
        "algo_usecase" : "Borders between clusters are fuzzy",
        "param_list" : {
            "i_iter" : {
              "param_info" : "The number of iterations you wish to obtain",
              "type" : "int",
              "default" : "1"
            },
            "m" : {
                "param_info" : "m is the weighting exponent. It controls the relative weights placed on each of the squared errors. The best value is between 1,5 and 3.",
                "type" : "float",
                "default" : 2
            },
            "error" : {
                "param_info" : "Stopping criterion. The algorithm stops if the norm of (u[p]-u[p-1] < error)" ,
                "type" : "float",
                "default" : 1.5
            },
            "maxiter" : {
                "param_info" : "maxiter is the maximum number of iterations allowed",
                "type" : "int",
                "default" : 1000
            },
            "n_clusters" : {
                "param_info" : "The number of clusters you wish to obtain",
                "type" : "int",
                "default" : 3
            },
            "score" : {
              "param_info" : "The score choose to calculate the best number of clusters. Calinski-Habaraz score is the relation between the sum of distances squared intergroup and the sum of distances squared intragroup. Whereas, Davies-Bouldin index is the relation between the sum of distances squared intragroup and the sum of distances squared intergroup. The aim is to minimize the sum of distances squared intragroup and to maximize the sum of distances squared intergroup. Smaller is the Davies-Bouldin index and bigger is the Calinski-Habaraz score, better is the number of clusters. Mean silhouette value is between -1 and 1 and the best value is around 1.",
              "type" : ["Calinski-Harabasz", "Davies-Bouldin", "Mean silhouette","Fuzzy partition coefficient"],
              "default" : "Mean silhouette"
            }
        }
    }
}
