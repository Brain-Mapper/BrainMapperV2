<!DOCTYPE html>
<html>
  <head>
    <title>User manual</title>
	<link rel="stylesheet" type="text/css" href="style.css">
  <link rel="icon" href="imgs/logo.png">
  </head>
  <body>
    <header class="w3-container w3-center w3-padding-32">
      <div class="row">
        <div class="column">
            <img src="imgs/logo.png" style="width:8%">
        </div>
        <div class="column">
          <h1><b>USER MANUAL</b></h1>
      	  <p>Welcome to the user manual for the <span class="w3-tag">BrainMapper</span> software.</p>
        </div>
      </div>
  	</header>

    <div class="row">
      <div class="column-left">
        <div class="w3-card w3-margin w3-margin-top">
          <div id="toc_container">
            <p class="toc_title">Contents - Clustering</p>
            <a href="index.html" style="color : #5577ee"> Go back to home page </a>
            <ul class="toc_list">
          	   <li><a href="#data"><strong>3.1 Data extraction for clustering</strong></a></li>
          	    <ul>
          		      <li><a href="#extract">3.1.1 How data are extracted</a></li>
          		      <li><a href="#all_points">3.1.2 Use all points of an image</a></li>
          		      <li><a href="#as_centroid">3.1.3 Use the image's points centroid as data</a></li>
          	      </ul>
              	<li><a href="#methods"><strong>3.2 Apply clustering algorithms on extracted data</strong></a></li>
              	  <ul>
              		<li><a href="#kmeans">3.2.1 KMeans</a></li>
              		<li><a href="#kmedoids">3.2.2 KMedoids</a></li>
                  <li><a href="#agglo">3.2.3 Agglomerative Clustering</a></li>
                  <li><a href="#dbscan">3.2.4 DBscan</a></li>
                  <li><a href="#fuzzyCmeans">3.2.3 Fuzzy Cmeans</a></li>
              	  </ul>

                <li><a href="#clust_results"><strong>3.3 Clustering results </strong></a></li>
                <ul>
                  <li><a href="#clust_assign">3.3.1 Cluster assignment</a></li>
                  <ul>
                    <li><a href="#as_set"><small>3.3.1.1 Save results as set</small></a></li>
                    <li><a href="#to_csv"><small>3.3.1.2 Export as CSV file</small></a></li>
                  </ul>

                  <li><a href="#val_index">3.3.2 Internal Validation Indexes</a></li>
                  <ul>
                    <li><a href="#mean_silhouette"><small>3.3.2.1 Mean Silhouette</small></a></li>
                    <li><a href="#ch_index"><small>3.3.2.2 Calinski-Habaraz Index</small></a></li>
                    <li><a href="#db_index"><small>3.3.2.3 Davies Boulin index</small></a></li>
                    <li><a href="#fpc"><small>3.3.2.4 Fuzzy Partition Coefficient</small></a></li>
                  </ul>

                  <li><a href="#visu">3.3.3 Graphic Visualisation</a></li>
                    <ul>
                      <li><a href="#silhouette_graph"><small>3.3.3.1 Silhouette graph</small></a></li>
                      <li><a href="#3d_view"><small>3.3.3.2 3D view</small></a></li>
                      <li><a href="#cross_sections"><small>3.3.3.3 Cross sections</small></a></li>
                      <li><a href="#glass_brain"><small>3.3.3.4 Glass brain</small></a></li>
                      <li><a href="#dendrogramm"><small>3.3.3.5 Dendrogramm</small></a></li>
                    </ul>

                  <li><a href="#nb_clusters">3.3.4 Choice the number of clusters</a></li>

                  <li><a href="#iteration">3.3.5 Iteration</a></li>

                </ul>
              </ul>
        	</div>
        </div>
      </div>
      <div class="column-right">
        <div id="Clustering">
          <h1> Clustering on NIfTI data</h1>
          <p>
          BrainMapper allows the user to use the interesting data from NIfTI files to perform clustering algorithms and thus determine the different groups of voxels.<br/>
          To accomplish this, BrainMapper extracts the data from NIfTI files or from CSV files and allows you to select the clustering method you would like to apply. Clustering results can be exported as a CSV file or
          saved in the application as a set.
          </p>
          <p>
          In this section we explain the main functionalities of our software around clustering.
          </p>

          <h2 id="data">3.1 Data extraction for clustering</h2>
        	<p>
        		The NIfTI format is an image format but for some teams it is interesting to apply clustering algorithms on the list of voxels, usually represented as a list of
            [X_coordinate,Y_coordinate,Z_coordinate, Intensity] entries, each of which represent a voxel.<br/>
            Our software extracts the data of your selected image collections before applying clustering algorithms on it.

        	</p>
        	<h3 id="extract">3.1.1 How data are extracted</h3>
        	<p>
            From the main view page, where all collections are accessible, the user can click on 'Clustering' button at the bottom right, once he has selected some image collections.<br/>

            <img src="imgs/clustering/access_clustering.png" style="width:80%">
            <br><br>
            A pop up dialog window will appear. It shows the number of image collections selected as well as the total number of NIfTI images to be treated.<br/>
            <img src="imgs/clustering/select_extraction_mode.png" style="width:80%">
            <br/><br/>
            In this windows, the user can select between two ways of extracting the image's information before the clustering view is loaded : our software creates a
            list of <em>interesting</em> voxels by extracting the coordinates of all the voxels that have an intensity greater than 0 or by calculating the image's centroid
            (each image will be represented by a single point).

        	</p>
          <h3 id="all_points">3.1.2 Use all points of an image</h3>
        	<p>
            When all points are selected by choosing 'Use all region points for each file', the data used for clustering is the list of all voxels whose intensity is greater than 0. <br/>
            The clustering view thus contains a data table with several data entries <br/>
            <img src="imgs/clustering/clust_all_pts.png" style="width:80%">
            <br/>

          </p>
          <h3 id="as_centroid">3.1.3 Use the image's points centroid as data</h3>
        	<p>
            By choosing 'Use centroids as file representation', the data used for clustering is the a list of a single voxel per file, which represents the <em>mean voxel</em> or <em>center</em>
            of all the voxels in the image. This type of extraction might take a while longer than the simple extraction, because several calculations are done.
            <!-- TODO : définir le centroide-->
            <br/>
            The clustering view thus contains a data table with a single data entries per selected file : if a total of 4 files in 2 different image collections were selected, the data table
              will display 4 data entries <br>
            <img src="imgs/clustering/clust_centroids.png" style="width:80%">
            <br>
          </p>

          <h2 id="methods">3.2 Apply clustering algorithms on extracted data</h2>
        	<p> Once you have extracted the data from the selected images, you can choose on which coordinates you want to apply a clustering algorithm.
            To be done, you have to select the columns on which you want work.
          </p>

          <p>On this following example, we have selected the coordinates X and Y.</p>

          <img src="imgs/clustering/selectColumns.png" style="width:80%">

          <p>
            Then, you can choose which clustering algorithm to apply by clicking on "Choose a clustering algorithm" at the top left in the clustering view.
          </p>

          <img src="imgs/clustering/clust_chooser.png" style="width:80%">
          <br>
          <br>
          The current version of <em>BrainMapper</em> has 5 clustering methods the user can choose from : <em><b>KMeans</b></em>, <em><b>KMedoids</b></em>, <em><b>Agglomerative Clustering</b></em>, <em><b>DBscan</b></em> and <em><b>Fuzzy CMeans</b></em>. <br>
          <em><b>KMeans</b></em>, <em><b>Agglomerative Clustering</b></em> and <em><b>DBscan</b></em> come from the library for machine learning in Python, <em><b>scikit-learn</b></em>
          (for more details click <a href="http://scikit-learn.org/stable/modules/clustering.html#clustering" target="_blank" style="color : #5577ee">here</a> ).
          <em><b>Fuzzy CMeans</b></em> comes from an another library, <em><b>skfuzzy</b></em>
          (for more details click <a href="https://pythonhosted.org/scikit-fuzzy/api/skfuzzy.cluster.html#cmeans" target="_blank" style="color : #5577ee">here</a> ).<br>
          An implementation of <em><b>KMedoids</b></em> was made available by the developping team.
          <br><br>
          Once an algorithm is selected, algorithm's parameters will appear on the left section of clustering view.
          You can select and enter the parameters for the clustering algorithm here.

          <p>
          <img src="imgs/clustering/clust_params.png" style="width:80%">
          </p>

          <p>If you need more information about the parameters, you can click on the "?" button.</p>

          <p>
            <img src="imgs/clustering/help.png" style="width:80%">
          </p>

          <p>A new window will appear to give more information about the parameter you have selected.</p>
          <p>
            <img src="imgs/clustering/parameters_information.png" style="width:80%">
          </p>


          <h3 id="kmeans">3.2.1 KMeans</h3>
        	<p>
            The KMeans algorithm is a classic clustering algorithm. <br>
            The aim is to group objects into groups centered on centroids.The objects are created using attributes.
            For this algorithm, it is necessary to choose the number of groups, the function of distance between two objects and the stopping condition. This algorithm has the advantage of being fast. However, the results depend on the initialization.<br>
            <br>We used the implementation from the <em><b>scikit-learn</b></em> library.<br>
            For more details on the algorithm and its parameters click <a href="http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeansl" target="_blank" style="color : #5577ee">here</a>
          </p>
          <p>
            <img src="imgs/clustering/Kmeans.png" style="width:70%">
          </p>

          <h3 id="kmedoids">3.2.2 KMedoids</h3>
        	<p>
            The KMedoids algorithm is an alternative to KMeans.<br>
            The aim is to group objects into groups centered on medoids.
            For this algorithm, it is necessary to choose the number of groups, the function of distance between two objects and the stopping condition.
            This algorithm has the advantage of being fast. However, the results depend on the initialization.<br>

            A configuration has a cost that is defined as the sum of distance between each point et the medoid of the group.
            At each iteration, we calculate if an another configuration (with another medoid) has a lower cost.<br>

            The Kmedoid algorithm is more resistant to noise than the Kmeans algorithm.
            But the number of calculations can be high if the number of points is high.
          </p>
          <p>
            <img src="imgs/clustering/Kmedoid.png" style="width:70%">
          </p>
          <h3 id="agglo">3.2.3 Agglomerative Clustering</h3>
        	<p>
            Agglomerative clustering is hierarchical clustering algorithm. <br><br>
            <br>
            At initialization, each object is in group composed only of itself.
            The algorithm iterates until a stopping condition is met (for example, the number of groups).
            At each iteration, the algorithm merges the two most similar groups.
            <br>
            The important parameter are the stopping condition and how to calculate the similarity between two groups.
            Three similarity mesures are often used :
            <ul>
              <li> Ward, variance after merging the two groups</li>
              <li> Average, average distance between objects in two groups</li>
              <li> Complete, maximum distance between objects in two groups</li>
            </ul>
            We prefer to use Ward in cases where groups are supposed of the same size.
            <br>
            This algorithm had the advantage of allowing the construction of a dendogramm.
            Nevertheless, it is very heavy in terms of calculations and can not manage non-convexe groups.
            <br><br>
            We used the implementation of this type of algorithm from the <em><b>scikit-learn</b></em> library.<br>
            For more details on its parameters click <a href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering" target="_blank" style="color : #5577ee">here</a>
          </p>
          <p>
            <img src="imgs/clustering/Agglo.png" style="width:70%">
          </p>

          <h3 id="dbscan">3.2.4 DBscan</h3>
          <p>
            This algorithm needs two inputs :
            <ul>
                      <li> ε, a positive real </li>
                      <li> min, a positive integer</li>
            </ul>
            An object is classified in one of those categories :
            <ul>
              <li> heart, id there are at least min objects in a radius ε aroud this object</li>
              <li> border, if there are fewer than min objects in a radius ε aroud this object and if there is at least a cluster core in a radius ε aroud this object</li>
              <li> noise, otherwise </li>
            </ul>
            This algorithm performs on many data. It allows to find groups complex and has a very good resistance to noise.
            The problem with this algorithm is that it is difficult to configure the parameters ε and min.
            <br><br>
            We used the implementation of this type of algorithm from the <em><b>scikit-learn</b></em> library.<br>
            For more details on its parameters click <a href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html" target="_blank" style="color : #5577ee">here</a>
         </p>
         <p>
           <img src="imgs/clustering/DBscan.png" style="width:70%">
         </p>

          <h3 id="fuzzyCmeans">3.2.5 Fuzzy CMeans</h3>
          <p>
            The principle is similar to that Kmeans.
            The difference is the value of belonging to a group that is not a binary value but a value between 0 and 1.
            The points can belong to several groups, which allows for more tailored results for groups with unclear boundaries.
            <br><br>
            We used the implementation of this type of algorithm from the <em><b>scikit-fuzzy</b></em> library.<br>
            For more details on its parameters click <a href="https://pythonhosted.org/scikit-fuzzy/api/skfuzzy.cluster.html#cmeans" target="_blank" style="color : #5577ee">here</a>
          </p>
          <p>
            <img src="imgs/clustering/Fuzzy.png" style="width:70%">
          </p>

          <h2 id="clust_results">3.3 Clustering results</h2>
        	<p> Once the selected clustering method parameters have been correctly set, you can launch the algorithm by clicking on the 'Run' button.</p>

          <img src="imgs/clustering/run.png" style="width:80%">

          <p>
            Cluster assignments will appear on the data table. Then, you could show graphics you want.
          </p>

          <div style="padding-left: 12%; padding-top:3%; padding-bottom:3%">
             <div class="alert note-info" role="alert">
             <h5 style="color:#306499;">NOTE</h5>
             <p>The final number of clusters can be inferior to the number of clusters chosen. That means that the algorithm has stopped
             before the end.</p>
           </div>
          </div>

          <h3 id="clust_assign">3.3.1 Cluster assignment</h3>
        	<p>
            The data table will be modified to display which data entry belongs to which cluster.
          </p>

          <p>
            <img src="imgs/clustering/assignment.png" style="width:80%">
          </p>

          <p>
            After launching a clustering algorithm, you have results in the result panel.
            In this panel, you have a reminder of the chosen parameters such as the value of max_iter, the score chosen, the number of clusters (n_clusters), the value of i_iter, the type of initialization and the coordinates selected.
            You also have the value of cluster centroids and the value of scores (Davies-Bouldin score, Calinski-Harabasz score and the mean silhouette).
          </p>

          <p>
            <img src="imgs/clustering/result_panel.png" style="width:80%">
          </p>

          <p>
            This assignment results can be saved in two ways : either by saving them as a new set in the application or by exporting
            the results as a CSV file.
          </p>

          <h4 id="as_set">3.3.1.1 Save as set</h4>
        	<p>
            By clicking on the "Save as set" button, a NIfTI file containing all points from a given cluster will be recreated,
            for each cluster obtained.
          </p>

          <p>
            <img src="imgs/clustering/save_as_set.png" style="width:80%">
          </p>

          <p>A set containing this results will be added in the main page, in the "Clustering" tab.</p>

          <p>
            <img src="imgs/clustering/clustering_main_view.png" style="width:80%">
          </p>

          <h4 id="to_csv">3.3.1.2 Export as CSV file</h4>
        	<p> By clicking on the "Export" button, a CSV file containing the data table can be saved on the disk.</p>

          <p>
            <img src="imgs/clustering/export.png" style="width:80%">
          </p>

          <h3 id="val_index">3.3.2 Internal Validation indexes</h3>
        	<p> Internal validation indexes can be useful when one needs to determine which clustering execution is to be retained as conclusive or which number of clusters is the most conclusive. <br><br>
            In this version of <em>BrainMapper</em>, validation indexes are calculated automatically after a clustering algorithm is applied on data.<br>
            The internal validation indexes of the current version are :
            <ul>
              <li>Mean Silhouette</li>
              <li>Calinski-Habaraz score</li>
              <li>Davis-Boulin index</li>
            </ul>

          <h4 id="mean_silhouette">3.3.2.1 Mean Silhouette</h4>
          <p>
            The aim of this method is to calculate for each point the similarity between him and his group by relation to the similarity between him and the other groups.
            Those data are displayed on a graph, sorting them by group and value.
            The closer the shape of each group is to a rectangle, the more the grouping represents the data.
            The silhouette value is between -1 and 1.
            The silhouette is correct if it respects the following criteria:
            <ul>
              <li>The average silhouette value is close to 1</li>
              <li>The groups are of equivalent size</li>
              <li>There are no objects with negative values</li>
              <li>The shape of a silhouette of a group is rectangular</li>
            </ul>
          </p>
          <p>
            <img src="imgs/clustering/MeanSilhouette.png" style="width:70%">
          </p>
        	<p> Mean silhouette index is calculated with the according function from <em><b>scikit-learn</b></em> library.
            For more details click <a href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html" target="_blank" style="color : #5577ee">here</a>.
          </p>
          <h4 id="ch_index">3.3.2.2 Calinski-Habaraz score</h4>
          <p>
            The Calinski-Harabasz index is based on the comparison of the sum of squares between clusters relative to the sum of the squares within a cluster.
            The aim is to maximize the measurement of cluster separation and minimize proximity measurement of points in a cluster.
            The index of Calinski-Harabasz is defined as follows :
            <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>
            $$ {CH =\frac{N-K}{K-1} * \frac{SS_B}{SS_W} } $$
            <br>
            with K the number of clusters, N the number of points, SS<sub>B</sub> the variance within a group and SS<sub>W</sub> between groups.
            <br>
            The higher the Calinski-Harabasz index, the better the number of groups.
          </p>
          <p>
            Calinski-Habaraz score is computed with the according function from <em><b>scikit-learn</b></em> library.
            For more details click <a href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.calinski_harabaz_score.html" target="_blank" style="color : #5577ee">here</a>.
          </p>
          <h4 id="db_index">3.3.2.3 Davis-Boulin index</h4>
          <p>
            Davies Bouldin index compares intra-cluster distances with inter-cluster distances.
            The principle is to have low inter-cluster distances and distances high inter-clusters. The Davies-Bouldin index is defined as follows:
            <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>
            $$ {DB =\frac{1}{K} \sum^K_{k=1}max \frac{I(c_i)+I(c_j)}{I(c_i,c_j)} } $$
            <br>
            with I(c<sub>i</sub>) the average of the distances between the objects of the group C<sub>i</sub> and its center,
            I(c<sub>i</sub>, c<sub>j</sub>) the distance between the centers of groups C<sub>i</sub> and C<sub>j</sub> and K the number of clusters
            <br>
            The lower the Davies Bouldin index, the better the number of groups.
          </p>
          <p>
            Davies Bouldin index score is computed with the according function from <em><b>scikit-learn</b></em> library.
            For more details click <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.davies_bouldin_score.html" target="_blank" style="color : #5577ee">here</a>.
          </p>

          <h4 id="fpc">3.3.2.3 Fuzzy Partition Coefficient</h4>
          <p>
            The Fuzzy Partition Coefficient (FPC) tells us how cleanly our data is described by the algorithm with the number of clusters chosen.
            This metric is defined between 0 and 1, with 1 being the best.
          </p>
          <p>
            FPC is a value returned by the Fuzzy CMeans algorithm from <em><b>scikit-fuzzy</b></em> library.
            For more details click <a href="https://pythonhosted.org/scikit-fuzzy/api/skfuzzy.cluster.html#cmeans" target="_blank" style="color : #5577ee">here</a>.
          </p>

          <div style="padding-left: 12%; padding-top:3%; padding-bottom:3%">
             <div class="alert note-info" role="alert">
             <h5 style="color:#306499;">NOTE</h5>
             <p>This metric can only be used with the Fuzzy CMeans algorithm.</p>
           </div>
          </div>


          <h3 id="visu">3.3.3 Graphic visualisation</h3>
        	<p>After launching a clustering algorithm, you can show different graphics.</p>
          <p>To show a graphic, you have to choose the graphic you want in the combo-box. Then, you have to select the button "Show".</p>

          <p>
            <img src="imgs/clustering/choose_graphic.png" style="width:80%">
          </p>

          <p>In this part, we describe all available graphics.</p>

            <h4 id="silhouette_graph">3.3.3.1 Silhouette graphic</h4>

            <p>If you select in the combo-box "Silhouette", you have the silhouette graphic.</p>
            <p>For all clustering algorithms, you can show the silhouette graphic. This one reprents the mean silhouette.</p>
            <p>For more detail about the mean silhouette, click <a href="#mean_silhouette">here</a>.</p>

            <p>
              <img src="imgs/clustering/silhouette.png" style="width:50%">
            </p>

            <h4 id="3d_view">3.3.3.2 3D view</h4>

            <p>If you select in the combo-box "3D view", you have a representation of clusters in 3D.</p>
            <p>Each cluster is represented by a different color and a different form. The centroid in each cluster is bigger than other points.</p>
            <p>In the following example, three clusters are represented in 3D.</p>

            <p>
              <img src="imgs/clustering/3d_view.png" style="width:60%">
            </p>

            <div style="padding-left: 12%; padding-top:3%; padding-bottom:3%">
               <div class="alert note-info" role="alert">
               <h5 style="color:#306499;">NOTE</h5>
               <p>This graphic is available for all clustering algorithms but for the Fuzzy CMeans algorithm, the color of points is the mix of clusters colors in terms of affiliations of those points to the different clusters.</p>
             </div>
            </div>

            <p>This following example shows the 3D view for the Fuzzy CMeans algorithm.</p>

            <p>
              <img src="imgs/clustering/3d_view_fuzzy.png" style="width:60%">
            </p>

            <h4 id="cross_sections">3.3.3.3 Cross sections</h4>

            <p>If you select in the combo-box "Cross sections", you have a representation of clusters in 2D.</p>
            <p>Each point is represented in 2D on a brain cross section. Each cluster has a different color.</p>

            <p>
              <img src="imgs/clustering/cross_sections.png" style="width:50%">
            </p>

            <p>For all clustering algorithms, you can show the silhouette graphic.</p>

            <h4 id="glass_brain">3.3.3.4 Glass brain</h4>

            <p>If you select in the combo-box "Glass brain", you have a representation of clusters in 2D on a glass brain cross section.</p>
            <p>Each point is represented in 2D on a brain cross section. Each cluster has a different color. Contrary to the cross section graphic, with the glass brain, you can see more points on the brain.</p>

            <p>
              <img src="imgs/clustering/glass_brain.png" style="width:50%">
            </p>

            <p>For all clustering algorithms, you can show the silhouette graphic.</p>

            <h4 id="dendrogramm">3.3.3.5 Dendrogram</h4>

            <p>The dendrogram helps in choosing the number of clusters. The higher is the division, the greater is the distance between the groups.</p>
            <p>On the following example, we could decide to create two or three clusters.</p>

            <p>
              <img src="imgs/clustering/dendro.png" style="width:70%">
            </p>

            <p>In the following example, you decided to make only two clusters (one in pink, one in purple).</p>

            <p>
              <img src="imgs/clustering/dendogram.png" style="width:50%">
            </p>

            <div style="padding-left: 12%; padding-top:3%; padding-bottom:3%">
              <div class="alert alert-info" role="alert">
              <h5 style="color:#f66;">WARNING</h5>
              <p>The dendrogram is only available for the agglomerative clustering.</p>
              </div>
            </div>

        <h3 id="nb_clusters">3.3.4 Choice of the number of clusters</h3>

        <p>BrainMapper can suggest a number of clusters for all clustering algorithms.</p>

        <p>To do, you have to mention an interval for the parameter n_clusters and to choose a score.
          Then, you have to select the buttoon "Run". And for this score, the algorithm returns the best value of clusters.</p>
        <p>In the following example, you have chosen the interval "3-6" for n_clusters and the Calinski-Harabasz score.</p>

        <p>
          <img src="imgs/clustering/choice_nb.png" style="width:80%">
        </p>

        <p>The result appers in the result panel.</p>
        <p>In the following example, the result is next to "n_selected". For this data, with the interval "3-6" for n_clusters and the Calinski Harabasz score,
          the best value for number of clusters is 4.
        </p>

        <p>
          <img src="imgs/clustering/result_nb.png" style="width:80%">
        </p>

        <div style="padding-left: 12%; padding-top:3%; padding-bottom:3%">
           <div class="alert note-info" role="alert">
           <h5 style="color:#306499;">NOTE</h5>
           <p>If we had chosen an other score, the result would have been different.</p>
         </div>
        </div>

        <p>We have redone the test changing only the score. With Davies Bouldin score, the best value is 3.</p>
        <p>
          <img src="imgs/clustering/diff_result.png" style="width:80%">
        </p>

        <h3 id="iteration">3.3.5 Iteration</h3>

        <p>With BrainMapper, you can launch a clustering algorithm several times.</p>
        <p>To do, you have to mention a value in i_iter. Then, you have to select the buttoon "Run".</p>
        <p>In the following example, we choose 3 for the value of i_iter and launch the algorithm. In this case, the Fuzzy CMeans algorithm will
        be launched thre times for each value of clusters (3 to 6) with Davies-Boulin score.</p>

        <p>
          <img src="imgs/clustering/iter.png" style="width:80%">
        </p>

        <p>In the result panel, we have the result of launching the Fuzzy CMeans algorithm three times.
        The yellow line shows the best result. And the blue line shows the result which is displayed in the part "Data inside the collection".
        If you select a column header ("I_number", "Clusters", "Mean silhouette", "Calinski-Harabasz", "Davies-Boulin" or "Fuzzy partition coefficient"),
        the rows will be ordered on their values in this column.
        </p>

        <p>
          <img src="imgs/clustering/result_iter.png" style="width:80%">
        </p>


          </div>
      </div>
    </div>
  </div>

  <footer class="w3-container w3-dark-grey w3-padding-32 w3-margin-top">
    <div class="column-left">
	     <p>Source code available on <a href="https://github.com/Brain-Mapper/brainMapper" target="_blank">GitHub repository</a></p>
    </div>
    <div>
      <p> <b>Authors</b>: R. Agathon (@yoshcraft), M. Cluchague (@maximeCluchague), G. Husson (@Graziella-Husson), V. Zelaya (@vz-chameleon),
      M. Adler (@MarieAdler), A. Benoit (@BenAur), T. Grassellini (@GrasselliniThomas), L. Martin (@MartinLucie)
    </div>
	</footer>
  </body>
</html>
